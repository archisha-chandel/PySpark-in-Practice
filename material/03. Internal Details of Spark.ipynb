{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internal Details of Spark\n",
    "#### Agenda\n",
    "* Understanding Cluster\n",
    "* Driver & Executors\n",
    "* Partitions\n",
    "* Spark Entities - Application, Job, Stages & Task\n",
    "* Resilient Distributed Datastructure\n",
    "* Spark DataFrames\n",
    "\n",
    "## 1. Understanding Cluster\n",
    "<hr>\n",
    "* In distributed computing environment, **systems have to be inter-connected for information exchange**. This inter-connected system is known as **cluster**. \n",
    "* Systems are connected using switch.\n",
    "* Cluster needs to be managed. \n",
    "* Spark have it's own **cluster manager**.\n",
    "* Other cluster managers are Mesos, YARN.\n",
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/23586e93bfb6846c5bbd0a5c14f2ed4a1e1ec40a/2-Figure1-1.png\" width=\"300px\">\n",
    "\n",
    "## 2. Driver & Executors\n",
    "<hr>\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/cluster-overview.png\">\n",
    "* Spark runs on cluster in deployments.\n",
    "* Spark follows a master-slave architecture.\n",
    "* Spark applications run as independent sets of processes on a cluster, coordinated by the program running in **driver node**.\n",
    "* **SparkContext** object is part of driver program & controls the spark processes.\n",
    "* For controlling processes across cluster SparkContext needs to use cluster managers (Sparks' Cluster Manager, YARN, Mesos).\n",
    "* One responsibility of cluster manager is to allocate resources in executors ( nodes in which data processing & storage happens )\n",
    "* Resources like (cpu,memory) gets allocated at **executor**, application code is sent.\n",
    "* Task runs on executor & is controlled by driver.\n",
    "* Driver program listens to executors.\n",
    "\n",
    "## 3. Data Partitioning\n",
    "<hr>\n",
    "* Data is splitted into partitions.\n",
    "* Each machine has more than one partitions.\n",
    "* Number of partitions per machine is configurable. Too-much or too-less may not be a great idea.\n",
    "* Partitions don't span across multiple machines.\n",
    "* Usual sizes of partitions are 64MB, 128MB or 256MB\n",
    "* By increasing number of partitioning, we can achieve higher parallelism.\n",
    "* But, network communication is expensive thing in distributed computing and manging too many tasks gets difficult.\n",
    "* Having right balance of partition maximizes usage.\n",
    "* Two ways of partitioning data\n",
    "  - HashPartition : Tries to evenly distribute data based on hash value.\n",
    "  - RangePartition : Tuples having keys of same range will appear in same partition.\n",
    "\n",
    "## 4. Spark Entities\n",
    "<hr>\n",
    "* **Application**- Complete user program built on Spark consisting of driver & executor code.\n",
    "* **Job** - An application when subjected to execution is known as job.\n",
    "* **Stages** - Job gets divided into stages. Same stage can be parallelized, but different stages are sequential.\n",
    "* **Task** - A stage in execution is known as task. Number of tasks at a time will be equal to number of partitions.\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*wiXLNwwMyWdyyBuzZnGrWA.png\" width=\"600px\">\n",
    "\n",
    "## 5. Resilient Distributed Datasets (RDDs)\n",
    "<hr>\n",
    "* Fundamental & Low-level data-structure around which spark revolves.\n",
    "* RDDs are immutable in nature.\n",
    "* RDD for a data is the mapping where the partition lies.\n",
    "\n",
    "    `rdd = sc.parallelize([1,2,3,4,5],2)`\n",
    "    \n",
    "    `rdd.glom().collect()`\n",
    "    \n",
    "<img src=\"https://github.com/awantik/machine-learning-slides/blob/master/rdd.PNG?raw=true\">\n",
    "\n",
    "## 6. Spark DataFrames\n",
    "<hr>\n",
    "* Distributed tabular data structure.\n",
    "* Spark dataframes is scalable unlike pandas dataframe.\n",
    "* They are immutable.\n",
    "* RDD has already gone into maintainance phase.\n",
    "* Spark Engine is responsible for generating optimized RDD from dataframes or spark-sql.\n",
    "\n",
    "    `df = spark.createDataFrame([(1,2),(2,3),(4,3),(7,8)],['A','B'])`\n",
    "    \n",
    "    `display(df)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
